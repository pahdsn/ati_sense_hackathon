{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Harrycode_pah.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pahdsn/ati_sense_hackathon/blob/master/Harrycode_pah.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0jnNBWOqBoI",
        "outputId": "919460b5-5bb3-47ed-d206-6822201af761"
      },
      "source": [
        "# Installations\r\n",
        "\r\n",
        "#!pip install geopandas\r\n",
        "#!pip install rasterio\r\n",
        "!pip install rasterstats"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rasterstats\n",
            "  Downloading https://files.pythonhosted.org/packages/9f/52/055b2b736e4aa1126c4619a561b44c3bc30fbe48025e6f3275b92928a0a0/rasterstats-0.15.0-py3-none-any.whl\n",
            "Requirement already satisfied: affine<3.0 in /usr/local/lib/python3.7/dist-packages (from rasterstats) (2.3.0)\n",
            "Requirement already satisfied: numpy>=1.9 in /usr/local/lib/python3.7/dist-packages (from rasterstats) (1.19.5)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.7/dist-packages (from rasterstats) (1.7.1)\n",
            "Collecting simplejson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/04/377418ac1e530ce2a196b54c6552c018fdf1fe776718053efb1f216bffcd/simplejson-3.17.2-cp37-cp37m-manylinux2010_x86_64.whl (128kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: rasterio>=1.0 in /usr/local/lib/python3.7/dist-packages (from rasterstats) (1.2.0)\n",
            "Requirement already satisfied: cligj>=0.4 in /usr/local/lib/python3.7/dist-packages (from rasterstats) (0.7.1)\n",
            "Requirement already satisfied: fiona in /usr/local/lib/python3.7/dist-packages (from rasterstats) (1.8.18)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from rasterio>=1.0->rasterstats) (20.3.0)\n",
            "Requirement already satisfied: snuggs>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from rasterio>=1.0->rasterstats) (1.4.7)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.7/dist-packages (from rasterio>=1.0->rasterstats) (1.1.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from rasterio>=1.0->rasterstats) (2020.12.5)\n",
            "Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.7/dist-packages (from rasterio>=1.0->rasterstats) (7.1.2)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.7/dist-packages (from fiona->rasterstats) (2.5.0)\n",
            "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.7/dist-packages (from fiona->rasterstats) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.7/dist-packages (from snuggs>=1.4.1->rasterio>=1.0->rasterstats) (2.4.7)\n",
            "Installing collected packages: simplejson, rasterstats\n",
            "Successfully installed rasterstats-0.15.0 simplejson-3.17.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hipR-oeqmYA"
      },
      "source": [
        "# Imports\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "import geopandas as gpd\r\n",
        "import pandas as pd\r\n",
        "import rasterio\r\n",
        "import os\r\n",
        "import glob\r\n",
        "from google.colab import drive\r\n",
        "import gdal\r\n",
        "from shapely.geometry import Point\r\n",
        "from PIL import Image\r\n",
        "from tqdm import tqdm\r\n",
        "from torch.utils.data import Dataset\r\n",
        "from torchvision import transforms\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "\r\n",
        "import fiona\r\n",
        "import rasterio\r\n",
        "import rasterio.mask\r\n",
        "from rasterio.plot import show\r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuygRzXXqpjW",
        "outputId": "483dd9c0-07bf-4ea9-f03e-ca075c866e61"
      },
      "source": [
        "# Mount Drive and set up paths\r\n",
        "\r\n",
        "drive.mount('/content/drive')\r\n",
        "#os.chdir('/content/drive/My Drive/Polar_Hack')\r\n",
        "os.chdir('/content/drive/My Drive/ati_sense_prac')\r\n",
        "SAMPLING_DIR = \"./samples/\"\r\n",
        "META_DIR = \"./samples_meta/\"\r\n",
        "SHAPEFILE_DIR = \"./EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/\" \r\n",
        "TIFF_DIR = \"./Sentinel geotiffs/\"\r\n",
        "subset_dir = SHAPEFILE_DIR+\"subset/\"\r\n",
        "\r\n",
        "shapefiles = glob.glob(SHAPEFILE_DIR+'*.shp') #[0:2]\r\n",
        "images = glob.glob(TIFF_DIR+'*.tif')\r\n",
        "shapefiles_subset = glob.glob(subset_dir+'*.shp')\r\n",
        "#print(subset_dir+'*.tif')\r\n",
        "print(shapefiles_subset)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "./EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/*.tif\n",
            "['./EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20181113t074529I.shp', './EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20181113t074529W.shp', './EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20180213t175444W.shp', './EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20180911t175548W.shp', './EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20181016t072958W.shp', './EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20180612t180423W.shp', './EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20181218t075437W.shp', './EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20180515t174633W.shp', './EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20180116t075430W.shp', './EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20180417t074606W.shp', './EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20180814t075344W.shp', './EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20180313t181225W.shp', './EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20180717t073809W.shp', './EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20180213t175444I.shp', './EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20180911t175548I.shp', './EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20181016t072958I.shp', './EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20180612t180423I.shp', './EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20181218t075437I.shp', './EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20180515t174633I.shp', './EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20180116t075430I.shp', './EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20180417t074606I.shp', './EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20180814t075344I.shp', './EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20180313t181225I.shp', './EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20180717t073809I.shp']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7R2ZUvm-8-Xm",
        "outputId": "5442c0f3-89eb-4151-d3e6-2aa92ca48e5a"
      },
      "source": [
        "#Just working out how to extract shape from image file\r\n",
        "#tiff0 =  gdal.Open(images[0])\r\n",
        "#tiff0a = gdal.Open(images[0]).ReadAsArray()\r\n",
        "#tiff1a = gdal.Open(images[-1]).ReadAsArray()\r\n",
        "#print(tiff0a.shape)\r\n",
        "#print(tiff1a.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 14179, 15598)\n",
            "(3, 15294, 15597)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaG0AkhHq90r"
      },
      "source": [
        "# Define routine for extracting label on subset of image\r\n",
        "\r\n",
        "def get_id(shapefile):\r\n",
        "  '''\r\n",
        "  Extracts datetime component of name\r\n",
        "  '''\r\n",
        "  return shapefile.split(\"_\")[-1][:-4].upper()\r\n",
        "\r\n",
        "def geo_ref(x,y,GT):\r\n",
        "  '''\r\n",
        "  return georeferenced point from pixel coordinates\r\n",
        "  '''\r\n",
        "  X_geo = GT[0] + x * GT[1] + y * GT[2]\r\n",
        "  Y_geo = GT[3] + x * GT[4] + y * GT[5]\r\n",
        "  return Point(X_geo, Y_geo)\r\n",
        "\r\n",
        "def sample(shapefile,x,y,N):\r\n",
        "  '''\r\n",
        "  Find tiff file, create NxN sample with origin (x,y) in pixel coordinates\r\n",
        "  Return id, sample and class from shapefile\r\n",
        "  '''\r\n",
        "  id = get_id(shapefile)\r\n",
        "  shape_data = gpd.read_file(shapefile)\r\n",
        "  tiff =  gdal.Open([g for g in images if id in g][0])\r\n",
        "  point = geo_ref(x+N/2,y+N/2,tiff.GetGeoTransform())\r\n",
        "  i=0\r\n",
        "  classification = None\r\n",
        "  while i < shape_data.shape[0] and classification == None:\r\n",
        "    if shape_data['geometry'][i].contains(point):\r\n",
        "      classification = shape_data['poly_type'][i]\r\n",
        "    i += 1\r\n",
        "  if classification != None:\r\n",
        "    im = Image.fromarray(np.transpose(tiff.ReadAsArray()[:,x:x+N,y:y+N],(1,2,0)))\r\n",
        "    image_name = SAMPLING_DIR+id+'X'+str(x)+'Y'+str(y)+'.png'\r\n",
        "    im.save(image_name)\r\n",
        "    return id, classification, image_name\r\n",
        "  else:\r\n",
        "    return None, None, None"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjIICGLC7y7O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b048eba2-86fb-4de8-93d5-5e663426e840"
      },
      "source": [
        "# Raw image dimensions\r\n",
        "i = 0 #change this to be the image you want\r\n",
        "tiff_as = gdal.Open(images[i]).ReadAsArray()\r\n",
        "xx = tiff_as.shape[2] #15564 \r\n",
        "yy = tiff_as.shape[1] #15218\r\n",
        "print(xx,yy)\r\n",
        "\r\n",
        "\r\n",
        "def get_samples(grid_space, sample_size, chunk_size):\r\n",
        "  '''\r\n",
        "  Grid space - how densely to sample the raw S1 images\r\n",
        "  sample size - size of square sample images (both in pixels)\r\n",
        "  Saves sample images in png format and metadata as csv file\r\n",
        "  '''\r\n",
        "  metadata = pd.DataFrame(columns=['id','x','y','label','image'])\r\n",
        "  for S in tqdm(shapefiles):\r\n",
        "    for x in np.arange(2*grid_space,xx-2*grid_space,grid_space):\r\n",
        "      for y in np.arange(2*grid_space,yy-2*grid_space,grid_space):\r\n",
        "        id, label, name = sample(S,x,y,sample_size)\r\n",
        "        if label != None:\r\n",
        "          metadata = metadata.append({'id':id,'x':x,'y':y,'label':label,'image':name},ignore_index=True)\r\n",
        "    metadata.to_csv(META_DIR+'samples.csv')\r\n",
        "  return metadata"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15598 14179\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PnUAvAeNw2F"
      },
      "source": [
        "#Trying to mask tifs based on masking shapefile shapes\r\n",
        "def sample2(i_shp):\r\n",
        "  '''\r\n",
        "  Find tiff file, create NxN sample with origin (x,y) in pixel coordinates\r\n",
        "  Return id, sample and class from shapefile\r\n",
        "  '''\r\n",
        "  \r\n",
        "  with fiona.open(i_shp, \"r\") as shapefile:\r\n",
        "      #shapes = [feature[\"geometry\"] for feature in shapefile]\r\n",
        "      shapes = [feature[\"geometry\"] for feature in shapefile]\r\n",
        "      #print(shapes) #= shapes['poly_type'=='I']\r\n",
        "  with rasterio.open(images[0]) as src:\r\n",
        "      ctype = str(i_shp[-5])\r\n",
        "      #could also loop through each shape and mask each individ shape\r\n",
        "      out_image, out_transform = rasterio.mask.mask(src, shapes, crop=True)\r\n",
        "      if ctype==\"I\":\r\n",
        "        k=1\r\n",
        "      elif ctype==\"W\":\r\n",
        "        k=2\r\n",
        "      elif ctype==\"L\":\r\n",
        "        k=3\r\n",
        "      out_image=k*out_image\r\n",
        "      out_meta = src.meta\r\n",
        "\r\n",
        "  return out_image, out_transform, out_meta"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBer4E1Sml5y",
        "outputId": "52ea89cc-773c-47b8-f519-47274b704cd5"
      },
      "source": [
        "# Raw image dimensions\r\n",
        "i = 0 #change this to be the image you want\r\n",
        "tiff_as = gdal.Open(images[i]).ReadAsArray()\r\n",
        "xx = tiff_as.shape[2] #15564 \r\n",
        "yy = tiff_as.shape[1] #15218\r\n",
        "print(xx,yy)\r\n",
        "\r\n",
        "def get_samples2(shapefiles):\r\n",
        "  '''\r\n",
        "  Pass it a shapefile_subset and have it return an array of masks w/ classifications\r\n",
        "  '''\r\n",
        "  out_images=[]\r\n",
        "  \r\n",
        "  \r\n",
        "  metadata = pd.DataFrame(columns=['id','x','y','label','image'])\r\n",
        "  \r\n",
        "  for S in tqdm(shapefiles):\r\n",
        "    pname, fname = os.path.split(S)\r\n",
        "    shapefile_subset = glob.glob(subset_dir+'*'+fname+'*.shp')\r\n",
        "    out_image_p=[]\r\n",
        "    for p in shapefile_subset:\r\n",
        "    #for x in np.arange(2*grid_space,xx-2*grid_space,grid_space):\r\n",
        "    #  for y in np.arange(2*grid_space,yy-2*grid_space,grid_space):\r\n",
        "      shapefile_subset[p]\r\n",
        "      out_image, out_transform, out_meta = sample2(shapefile_subset)\r\n",
        "    #    if label != None:\r\n",
        "      out_image_p = out_image_p + out_image\r\n",
        "    #out_images = out_image_p + out_image\r\n",
        "    #metadata = metadata.append({'id':id,'x':x,'y':y,'label':label,'image':name},ignore_index=True)\r\n",
        "    #metadata.to_csv(META_DIR+'samples.csv')\r\n",
        "    out_images=[out_images,out_image_p]\r\n",
        "  return out_images"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15598 14179\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBg1ht6dokH2",
        "outputId": "5ce644e3-c114-43cf-9e6b-c759d5b8ceab"
      },
      "source": [
        "out_images = get_samples2(shapefiles)\r\n",
        "out_images"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "100%|██████████| 12/12 [00:00<00:00, 443.06it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[[[[[[[[[[[], []], []], []], []], []], []], []], []], []], []], []], []]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rfL9mcDDnsw",
        "outputId": "b4c7694a-26fd-4523-9879-808661bfac9d"
      },
      "source": [
        "inum = 0 #change this to be the image you wwant to get the xx, xy for\r\n",
        "csfn = get_id(shapefiles[inum])\r\n",
        "tiff_as = gdal.Open(images[i]).ReadAsArray()\r\n",
        "xx = tiff_as.shape[2] #15564 \r\n",
        "yy = tiff_as.shape[1] #15218\r\n",
        "print(xx,yy)\r\n",
        "#point = geo_ref(x,y,GT)\r\n",
        "#id, classification, image_name = sample(shapefile,x,y,N)\r\n",
        "grid_space = 1000\r\n",
        "sample_size = 32\r\n",
        "metadata = get_samples(grid_space, sample_size)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "15598 14179\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 50%|█████     | 1/2 [03:17<03:17, 197.43s/it]\u001b[A\n",
            "100%|██████████| 2/2 [06:29<00:00, 194.91s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L80o_tD78lBG",
        "outputId": "8464328c-1beb-4b82-9d98-3ffa8ab22cdd"
      },
      "source": [
        "# Raw image dimensions\r\n",
        "tiff_as = gdal.Open(images[i]).ReadAsArray()\r\n",
        "xxi = tiff_as.shape[2] #15564 \r\n",
        "yyi = tiff_as.shape[1] #15218\r\n",
        "print(xxi, yyi)\r\n",
        "#500pixel spacing \r\n",
        "#sample size 64\r\n",
        "#4713 samples\r\n",
        "\r\n",
        "subset_dir = SHAPEFILE_DIR+\"subset/\"\r\n",
        "i_shp = subset_dir + \"seaice_s1_20181113t074529I.shp\"\r\n",
        "ctype = i_shp[-5]\r\n",
        "print(ctype)\r\n",
        "\r\n",
        "with fiona.open(i_shp, \"r\") as shapefile:\r\n",
        "    #shapes = [feature[\"geometry\"] for feature in shapefile]\r\n",
        "    shapes = [feature[\"geometry\"] for feature in shapefile]\r\n",
        "    #print(shapes) #= shapes['poly_type'=='I']\r\n",
        "with rasterio.open(images[0]) as src:\r\n",
        "    #ctype = i_shp\r\n",
        "    out_image, out_transform = rasterio.mask.mask(src, shapes, crop=True)\r\n",
        "    out_meta = src.meta\r\n",
        "print(out_image)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15598 14179\n",
            "I\n",
            "[[[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ihj4027rPj4j",
        "outputId": "828ae303-1173-422e-9b2b-39d4e99c3cb2"
      },
      "source": [
        "i=0\r\n",
        "shape_data = gpd.read_file(shapefiles[i])\r\n",
        "pname, fname = os.path.split(shapefiles[i])\r\n",
        "print(fname)\r\n",
        "print(shapefiles[i])\r\n",
        "#shape_data['geometry'][0]\r\n",
        "#shape_data['geometry'][1]\r\n",
        "#from rasterstats import zonal_stats\r\n",
        "#listofzones = zonal_stats(shapefiles[0], images[0],\r\n",
        "#            stats=\"mean\")\r\n",
        "#listofzones\r\n",
        "def subset_by_class(shapefiles, shape_data, ctype):\r\n",
        "  '''\r\n",
        "  Basically this creates shapefiles w/ just 1 class from each of the origional shapefiles\r\n",
        "  '''\r\n",
        "  #for ctype in ['I', 'L', 'W']:\r\n",
        "  for i in range(0,len(shapefiles)):\r\n",
        "    shape_data = gpd.read_file(shapefiles[i])\r\n",
        "    pname, fname = os.path.split(shapefiles[i])\r\n",
        "    shape_data_s = shape_data.copy()\r\n",
        "    shape_data_s=shape_data_s[shape_data_s['poly_type']==ctype]\r\n",
        "    #shape_data\r\n",
        "    #[feature[\"geometry\"] for feature in shape_data]\r\n",
        "    shape_data_s.to_file(SHAPEFILE_DIR+\"subset/\"+fname[0:-4]+ctype+\".shp\")\r\n",
        "    print(SHAPEFILE_DIR+\"subset/\"+fname[0:-4]+ctype+\".shp\")\r\n",
        "\r\n",
        "subset_by_class(shapefiles, shape_data, 'I')"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "seaice_s1_20181113t074529.shp\n",
            "./EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/seaice_s1_20181113t074529.shp\n",
            "./EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20181113t074529I.shp\n",
            "./EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20180213t175444I.shp\n",
            "./EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20180911t175548I.shp\n",
            "./EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20181016t072958I.shp\n",
            "./EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20180612t180423I.shp\n",
            "./EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20181218t075437I.shp\n",
            "./EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20180515t174633I.shp\n",
            "./EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20180116t075430I.shp\n",
            "./EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20180417t074606I.shp\n",
            "./EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20180814t075344I.shp\n",
            "./EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20180313t181225I.shp\n",
            "./EE_Polar_Training_Dataset_v-1-0-0/Sea_Ice/subset/seaice_s1_20180717t073809I.shp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "KCOR5ryW_K8h",
        "outputId": "86fb6a66-4984-431d-9ade-999c1c627698"
      },
      "source": [
        "\r\n",
        "LABELS = {\r\n",
        "\t\"L\": 0,\r\n",
        "\t\"W\": 1,\r\n",
        "\t\"I\": 2,\r\n",
        "}\r\n",
        "samples = pd.read_csv(META_DIR+'samples.csv',usecols=['id','x','y','label','image'])\r\n",
        "samples['label'] = [LABELS.get(ll) for ll in samples['label']]\r\n",
        "samples.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-6294e880d464>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m\"I\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m }\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMETA_DIR\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'samples.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mLABELS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mll\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mll\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './samples_meta/samples.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9Hj-rpXEPvv"
      },
      "source": [
        "samples.values[:,4]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkee63SCFFvL"
      },
      "source": [
        "# Set up data for torch\r\n",
        "\r\n",
        "TRAIN_SIZE = 0.7\r\n",
        "\r\n",
        "class PolarPatch(Dataset):\r\n",
        "    '''\r\n",
        "    TorchUtils dataset\r\n",
        "    '''\r\n",
        "    def __init__(self, transform=None, split=\"train\"):\r\n",
        "        super(PolarPatch, self).__init__()\r\n",
        "\r\n",
        "        assert split in [\"train\", \"val\"]\r\n",
        "        \r\n",
        "        # TODO: load in meta data, which should be of shape (3, N) - N being the number of samples\r\n",
        "        meta = samples.values\r\n",
        "\r\n",
        "        train_dim = int(TRAIN_SIZE * len(meta))\r\n",
        "        \r\n",
        "        if split == \"train\":\r\n",
        "            meta = meta[:train_dim]\r\n",
        "        else:\r\n",
        "            meta = meta[train_dim:]                   \r\n",
        "        self.images = meta[:,4]\r\n",
        "        self.coords = [(row[1], row[2]) for row in meta]\r\n",
        "\r\n",
        "        # Targets in integer form for computing cross entropy\r\n",
        "        self.targets = meta[:,3]\r\n",
        "        self.transform = transform\r\n",
        "\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.targets)\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "\r\n",
        "        x = Image.open(self.images[index]) # change this file format if needed\r\n",
        "        y = self.targets[index]\r\n",
        "        coord = self.coords[index]\r\n",
        "\r\n",
        "        if self.transform:\r\n",
        "        \tx = self.transform(x)\r\n",
        "\r\n",
        "        return x, y, coord\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tiu21FMIM-pj"
      },
      "source": [
        "BATCH_SIZE = 128\r\n",
        "\r\n",
        "data_transform = transforms.Compose([\r\n",
        "    # TODO: add whatever else you need - normalisation, augmentation, etc.\r\n",
        "\ttransforms.ToTensor(),\r\n",
        "])\r\n",
        "\r\n",
        "train_set = PolarPatch(\r\n",
        "    split='train',\r\n",
        "    transform=data_transform\r\n",
        ")\r\n",
        "\r\n",
        "train_loader = torch.utils.data.DataLoader(\r\n",
        "    train_set,\r\n",
        "    batch_size=BATCH_SIZE,\r\n",
        "    shuffle=True,\r\n",
        "    num_workers=2\r\n",
        ")\r\n",
        "\r\n",
        "test_set = PolarPatch(\r\n",
        "    split='val',\r\n",
        "    transform=data_transform\r\n",
        ")\r\n",
        "\r\n",
        "test_loader = torch.utils.data.DataLoader(\r\n",
        "    test_set,\r\n",
        "    batch_size=BATCH_SIZE,\r\n",
        "    shuffle=False,\r\n",
        "    num_workers=2\r\n",
        ")\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqnI8IJhQyxm"
      },
      "source": [
        "# nn i torch neural network class\r\n",
        "# Create a specific nn for our purposes called PolarNet\r\n",
        "\r\n",
        "class PolarNet(nn.Module): # nn.Module is base class for all networks\r\n",
        "    def __init__(self, n_classes=3):\r\n",
        "        super(PolarNet, self).__init__()\r\n",
        "# Super means inherit attributes, presumably from nn.Module\r\n",
        "        self.features = nn.Sequential(\r\n",
        "            # TODO: build your own architecture here; one conv layer and ReLU here as an example only\r\n",
        "            nn.Conv2d(3, 16, kernel_size=5, padding=0),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.MaxPool2d(kernel_size=(2,2),stride=2),\r\n",
        "            nn.Conv2d(16,32,kernel_size=5,padding=0),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.MaxPool2d(kernel_size=(2,2),stride=2)\r\n",
        "        ) # 13*13 * 32 = 5408 pixels\r\n",
        "\r\n",
        "        self.classifier = nn.Sequential(\r\n",
        "            nn.Linear(5408,36),\r\n",
        "            nn.ReLU(), \r\n",
        "            nn.Linear(36, 3)\r\n",
        "        )      \r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        # as an example; alter as needed depending on your architecture\r\n",
        "        x = self.features(x)\r\n",
        "\r\n",
        "        x = torch.flatten(x, 1)\r\n",
        "        x = self.classifier(x)\r\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjZmQKe1RI-_"
      },
      "source": [
        "# Device configuration - defaults to CPU unless GPU is available on device\r\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\r\n",
        "\r\n",
        "model = PolarNet().to(DEVICE)\r\n",
        "loss_fn = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "# Stochastic Gradient Descent\r\n",
        "optimizer = torch.optim.SGD(\r\n",
        "\tmodel.parameters(),\r\n",
        "\tlr=0.001,\r\n",
        "\tweight_decay=0.0005,\r\n",
        "\tmomentum=0.9,\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS-8RUILgePp"
      },
      "source": [
        "'''\r\n",
        "David Hogg's functions (modified) for capturing the stats during training\r\n",
        "'''\r\n",
        "\r\n",
        "def stats(loader, net):\r\n",
        "    correct = 0\r\n",
        "    total = 0\r\n",
        "    running_loss = 0\r\n",
        "    n = 0\r\n",
        "    with torch.no_grad():\r\n",
        "        for data in loader:\r\n",
        "            images, labels, coords = data\r\n",
        "            images = images.to(DEVICE)\r\n",
        "            labels = labels.to(DEVICE)\r\n",
        "            outputs = net.forward(images)\r\n",
        "            loss = loss_fn(outputs, labels)\r\n",
        "            _, predicted = torch.max(outputs.data, 1)\r\n",
        "            total += labels.size(0)    # add in the number of labels in this minibatch\r\n",
        "            correct += (predicted == labels).sum().item()  # add in the number of correct labels\r\n",
        "            running_loss += loss\r\n",
        "            n += 1\r\n",
        "    return running_loss/n, correct/total \r\n",
        "\r\n",
        "def statsplot(statsrec,name):\r\n",
        "    fig, ax1 = plt.subplots()\r\n",
        "    plt.plot(statsrec[0], 'r', label = 'training loss', )\r\n",
        "    plt.plot(statsrec[1], 'g', label = 'test loss' )\r\n",
        "    plt.legend(loc='center')\r\n",
        "    plt.xlabel('epoch')\r\n",
        "    plt.ylabel('loss')\r\n",
        "    plt.title('Training and test loss, and test accuracy')\r\n",
        "    ax2=ax1.twinx()\r\n",
        "    ax2.plot(statsrec[2], 'b', label = 'test accuracy')\r\n",
        "    ax2.set_ylabel('accuracy')\r\n",
        "    plt.legend(loc='upper left')\r\n",
        "    plt.savefig('./figures/'+name+'.png')\r\n",
        "    plt.show()\r\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DIScPmLgQRQ"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYxgRRlhfnuH"
      },
      "source": [
        "nepochs = 10\r\n",
        "# How many times go through the full training dataset\r\n",
        "\r\n",
        "statsrec = np.zeros((3,nepochs))\r\n",
        "# Numy array for holding some results\r\n",
        "\r\n",
        "for epoch in range(nepochs):  # loop over the dataset multiple times\r\n",
        "\r\n",
        "    running_loss = 0.0\r\n",
        "    n = 0\r\n",
        "    for i, data in enumerate(train_loader, 0):\r\n",
        "        inputs, labels, coords = data\r\n",
        "        inputs = inputs.to(DEVICE)\r\n",
        "        labels = labels.to(DEVICE)\r\n",
        "         # Zero the parameter gradients\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        # Forward, backward, and update parameters\r\n",
        "        outputs = model.forward(inputs)\r\n",
        "        loss = loss_fn(outputs, labels)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "    \r\n",
        "        # accumulate loss\r\n",
        "        running_loss += loss.item()\r\n",
        "        n += 1\r\n",
        "    \r\n",
        "    ltrn = running_loss/n\r\n",
        "    ltst, atst = stats(test_loader, model)\r\n",
        "    statsrec[:,epoch] = (ltrn, ltst, atst)\r\n",
        "    print(f\"epoch: {epoch} training loss: {ltrn: .3f}  test loss: {ltst: .3f} test accuracy: {atst: .1%}\")\r\n",
        "\r\n",
        "print('********** Finished Training ***************')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkpkyJfWh4Ru"
      },
      "source": [
        "statsplot(statsrec,'5408_secondrun')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}