{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "ati_sense_hackathon_pah.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN63DQcFi2jY"
      },
      "source": [
        "#!pip install rasterio #geopandas"
      ],
      "id": "DN63DQcFi2jY",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysvWksj7kIgW",
        "outputId": "1edca537-12ed-4e16-d7c7-25bd0c3dc87c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#If on collab\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/gdrive')\r\n",
        "drive.mount('/content/drive')\r\n",
        "import os\r\n",
        "os.chdir('/content/drive/My Drive/ati_sense_prac')"
      ],
      "id": "ysvWksj7kIgW",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "scheduled-survivor",
        "outputId": "119f9374-c2b1-4093-c481-7f88a119deaa"
      },
      "source": [
        "#import json\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "#import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from sys import path\n",
        "#import time\n",
        "#from tqdm import tqdm\n",
        "import glob\n",
        "import rasterio\n",
        "from rasterio.plot import show\n",
        "\n",
        "%matplotlib inline\n",
        "os.getcwd()\n",
        "os.path.dirname(os.getcwd())"
      ],
      "id": "scheduled-survivor",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "demographic-technician"
      },
      "source": [
        "# Automated Sentinel-1 Ice, Water, Land Segmentation Challenge\n",
        "\n",
        "\n",
        "\n",
        "This notebook is the adjusted template, to help guide through the training process. Feel free to use as little or as much of it as you like.\n",
        "\n",
        "For the purposes of the template, we will assume a *classification* approach, which involves sub-sampling small images from the Sentinel-1 images. There will be notes where code should be adjusted for a *segmentation* approach."
      ],
      "id": "demographic-technician"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "operational-comparative"
      },
      "source": [
        "### Dataset preparation - (1) sub-sampling\n",
        "\n",
        "Sample patches from each TIF image, and find the corresponding label using the Shapefiles. Save each image with a unique ID save in the directory **SAMPLING_DIR**. Save the corresponding meta data in the following format (this could be a CSV file, NumPy array, or some other format), in the directory **META_DIR**:\n",
        "\n",
        "\n",
        "```\n",
        "image_id, x, y, label\n",
        "```\n",
        "\n",
        "Set the label value as one of \"L\", \"W\", \"I\" as specified in the Shapefiles.\n",
        "\n",
        "To make it easier to patch the final segmentation back together, it is suggested to use the (x, y) pixel coordinates of the patch, rather than the spatial coordinates."
      ],
      "id": "operational-comparative"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "humanitarian-superintendent",
        "outputId": "10894b8f-ef7a-466e-b86b-7435ba2edefe"
      },
      "source": [
        "curdir = os.path.dirname(os.getcwd())\n",
        "SAMPLING_DIR = curdir + '\\sampling_dir' \n",
        "META_DIR = curdir + '\\meta_dir'\n",
        "print(SAMPLING_DIR, META_DIR)"
      ],
      "id": "humanitarian-superintendent",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\\sampling_dir /content/drive/My Drive\\meta_dir\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suffering-butter"
      },
      "source": [
        "Some helpful code: reading in a single Sentinel-1 image and the corresponding Shapefile."
      ],
      "id": "suffering-butter"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "earlier-hebrew",
        "outputId": "df404329-7403-44c7-f26f-ae2883adb3d5"
      },
      "source": [
        "# the directory containing all shapefiles - i.e., the location of sea_ice/ \n",
        "SHAPEFILE_DIR = curdir + '\\EE_Polar_Training_Dataset_v-1-0-0'  \n",
        "\n",
        "shapefile = SHAPEFILE_DIR + '\\Sea_Ice\\seaice_s1_20180116t075430.shp' # full name of .shp file\n",
        "\n",
        "print(shapefile)\n",
        "# extract the shape ID, for example, 20180116T075430\n",
        "shp_id = shapefile.split(\"_\")[-1][:-4].upper()\n",
        "print(shp_id)\n",
        "\n",
        "# locate the corresponding Sentinel-1 image based on the ID\n",
        "# this should only return 1 match, which you can confirm\n",
        "tiff_file = curdir + '\\Sentinel geotiffs\\S1?_*_' + shp_id + '*.tif'\n",
        "print(tiff_file)\n",
        "#S1B_EW_GRDM_1SDH_20181113T074529_20181113T074629_013583_019254_D382_Orb_Cal_Spk_TC_rgb_8bit\n",
        "\n",
        "#for name in glob.glob(tiff_file):\n",
        "#    print(name)\n",
        "\n",
        "tiff_file = glob.glob(tiff_file)[0]\n",
        "print(tiff_file)\n",
        "\n",
        "#tiff_file = #[g for g in tiff_files if shp_id in g]\n",
        "#tiff_file = tiff_file[0]"
      ],
      "id": "earlier-hebrew",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\\EE_Polar_Training_Dataset_v-1-0-0\\Sea_Ice\\seaice_s1_20180116t075430.shp\n",
            "20180116T075430\n",
            "/content/drive/My Drive\\Sentinel geotiffs\\S1?_*_20180116T075430*.tif\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-e855772eb869>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#    print(name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtiff_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtiff_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtiff_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "valid-entity"
      },
      "source": [
        "Feel free to use other Python packages; but as an example, here we use **geopandas** to read in the Shapefile, and **rasterio** to read the GeoTIFF."
      ],
      "id": "valid-entity"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "figured-allergy"
      },
      "source": [
        "#shape_data = gpd.read_file(SHAPEFILE_DIR + shapefile)\n",
        "shape_data = gpd.read_file(shapefile)\n",
        "\n",
        "shape_data.head()"
      ],
      "id": "figured-allergy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prime-corrections"
      },
      "source": [
        "shape_data['geometry'][100]\n",
        "#print(shape_data['geometry'][1].centroid)\n",
        "#print(shape_data['geometry'].head())\n",
        "np.nanmin(shape_data['geometry'])"
      ],
      "id": "prime-corrections",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "returning-culture"
      },
      "source": [
        "# directory containing all GeoTIFF files\n",
        "TIFF_DIR = curdir + '\\Sentinel geotiffs'  \n",
        "\n",
        "#tif_img = rasterio.open(TIFF_DIR + tiff_file)\n",
        "tif_img = rasterio.open(tiff_file)\n",
        "#tif_img\n",
        "#plt.imshow(tif_img.read(tif_img), cmap='pink')\n",
        "#plt.show()\n",
        "show(tif_img)\n",
        "show(tif_img.read(), transform=tif_img.transform)"
      ],
      "id": "returning-culture",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "false-rochester"
      },
      "source": [
        ""
      ],
      "id": "false-rochester",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rotary-austin"
      },
      "source": [
        ""
      ],
      "id": "rotary-austin",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amazing-legislation"
      },
      "source": [
        ""
      ],
      "id": "amazing-legislation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "running-syntax"
      },
      "source": [
        "The shapes in the Shapefiles are **shapely** objects. We can also use the Python package **shapely** to check whether an x, y pixel coordinate position is in a given polyshape."
      ],
      "id": "running-syntax"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "confident-proceeding"
      },
      "source": [
        "from shapely.geometry import Point\n",
        "#-489524.300 -1426091.270\n",
        "x = -214683.0164737697 \n",
        "y = -1599535.861627199\n",
        "\n",
        "point = Point(x, y)\n",
        "\n",
        "# for example, specify the shape in the Shapefile\n",
        "shape_id = 1\n",
        "\n",
        "if shape_data['geometry'][shape_id].contains(point):\n",
        "    print(\"Point\", point, \"is in shape\", shape_id, \"and has class\", shape_data['poly_type'][shape_id])\n"
      ],
      "id": "confident-proceeding",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cellular-malta"
      },
      "source": [
        ""
      ],
      "id": "cellular-malta",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "confirmed-details"
      },
      "source": [
        ""
      ],
      "id": "confirmed-details",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "popular-usage"
      },
      "source": [
        "Define a train/validation ratio. Patches and meta saved from the test TIF images should be stored in separate directories."
      ],
      "id": "popular-usage"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roman-discrimination"
      },
      "source": [
        "TRAIN_SIZE = 0.7\n",
        "\n",
        "# valid size = 1.0 - TRAIN_SIZE"
      ],
      "id": "roman-discrimination",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "parental-canadian"
      },
      "source": [
        "Map the class category characters to integers."
      ],
      "id": "parental-canadian"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "greatest-virgin"
      },
      "source": [
        "LABELS = {\n",
        "\t\"L\": 0,\n",
        "\t\"W\": 1,\n",
        "\t\"I\": 2,\n",
        "}"
      ],
      "id": "greatest-virgin",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "interpreted-addition"
      },
      "source": [
        "The following is a Dataset class which reads in image data saved in the format described above."
      ],
      "id": "interpreted-addition"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "environmental-baker"
      },
      "source": [
        "import torch \n",
        "import torchvision\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class PolarPatch(Dataset):\n",
        "    def __init__(self, transform=None, split=\"train\"):\n",
        "        super(PolarPatch, self).__init__()\n",
        "\n",
        "        assert split in [\"train\", \"val\"]\n",
        "        \n",
        "        # TODO: load in meta data, which should be of shape (3, N) - N being the number of samples\n",
        "        meta = []\n",
        "\n",
        "        train_dim = int(TRAIN_SIZE * len(meta))\n",
        "        \n",
        "        if split == \"train\":\n",
        "            meta = meta[:train_dim]\n",
        "        else:\n",
        "            meta = meta[train_dim:]                   \n",
        "\n",
        "        self.images = range(len(meta))\n",
        "        self.coords = [(row[1], row[2]) for row in meta]\n",
        "\n",
        "        # Targets in integer form for computing cross entropy\n",
        "        self.targets = [LABELS[row[3]] for row in meta]\n",
        "        self.transform = transform\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        x = Image.open(SAMPLING_DIR + str(self.images[index]) + \".png\") # change this file format if needed\n",
        "        y = self.targets[index]\n",
        "        coord = self.coords[index]\n",
        "\n",
        "        if self.transform:\n",
        "        \tx = self.transform(x)\n",
        "\n",
        "        return x, y, coord"
      ],
      "id": "environmental-baker",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reasonable-excerpt"
      },
      "source": [
        "An example data transform"
      ],
      "id": "reasonable-excerpt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "israeli-meditation"
      },
      "source": [
        "data_transform = transforms.Compose([\n",
        "    # TODO: add whatever else you need - normalisation, augmentation, etc.\n",
        "\ttransforms.ToTensor(),\n",
        "])"
      ],
      "id": "israeli-meditation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "virtual-spoke"
      },
      "source": [
        "### Dataset preparation - (2) data loaders\n",
        "\n",
        "Now we can prepare the data loaders. Here is the example for the training set; you will also need the validation and test set."
      ],
      "id": "virtual-spoke"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emerging-details"
      },
      "source": [
        "import torch\n",
        "\n",
        "# TODO set this value based on your working environment\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_set = PolarPatch(\n",
        "    split='train',\n",
        "    transform=data_transform\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "#Added from other day\n",
        "test_set = torchvision.datasets.PolarPatch(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_set,\n",
        "    batch_size=24, # Forward pass only so batch size can be larger\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "classes = (\"L\", \"W\", \"I\")"
      ],
      "id": "emerging-details",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "underlying-local"
      },
      "source": [
        "#can disp images using imshow BUT our things have rgb channel at beginning not the end\n",
        "def timshow(x):\n",
        "    xa = np.transpose(x.numpy(),(1,2,0))\n",
        "    plt.imshow(xa)\n",
        "    plt.show()\n",
        "    \n",
        "# get some random training images using the data loader\n",
        "dataiter = iter(train_loader) #iter creates iterator from it (so if you say next you get the next mini batch)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images and labels\n",
        "timshow(torchvision.utils.make_grid(images)) # torch vision makes a nice image from it\n",
        "\n",
        "print(f\"labels {[classes[labels[i]] for i in range(10)]}\")\n",
        "#labels = labels of each image\n",
        "#images = single tensor (16x3x32x32) as has 16 3d structures rep color images"
      ],
      "id": "underlying-local",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "religious-international"
      },
      "source": [
        "#this creates graph sturctures on the data\n",
        "def stats(loader, net):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0\n",
        "    n = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)    # add in the number of labels in this minibatch\n",
        "            correct += (predicted == labels).sum().item()  # add in the number of correct labels\n",
        "            running_loss += loss\n",
        "            n += 1\n",
        "    return running_loss/n, correct/total \n",
        "\n",
        "def statsplot(statsrec):\n",
        "    fig, ax1 = plt.subplots()\n",
        "    plt.plot(statsrec[0], 'r', label = 'training loss', )\n",
        "    plt.plot(statsrec[1], 'g', label = 'test loss' )\n",
        "    plt.legend(loc='center')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.title('Training and test loss, and test accuracy')\n",
        "    ax2=ax1.twinx()\n",
        "    ax2.plot(statsrec[2], 'b', label = 'test accuracy')\n",
        "    ax2.set_ylabel('accuracy')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.show()"
      ],
      "id": "religious-international",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "automotive-stock"
      },
      "source": [
        "### Model\n",
        "\n",
        "You can use a custom model architecture, or copy one from literature. It is recommended to not build too deep of a network for the sake of training time."
      ],
      "id": "automotive-stock"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stunning-length"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class PolarNet(nn.Module):\n",
        "    def __init__(self, n_classes=3):\n",
        "        super(PolarNet, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            # TODO: build your own architecture here; one conv layer and ReLU here as an example only\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True), \n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            # TODO: continue classifier section of architecture here for classification approach;\n",
        "            # otherwise, remove and add in upscaling for a fully-convolutional segmentation approach \n",
        "            nn.Linear(4096, n_classes),\n",
        "        )      \n",
        "\n",
        "    def forward(self, x):\n",
        "        # as an example; alter as needed depending on your architecture\n",
        "        x = self.features(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "id": "stunning-length",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bizarre-laugh"
      },
      "source": [
        ""
      ],
      "id": "bizarre-laugh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alpha-custom"
      },
      "source": [
        "### Training\n",
        "\n",
        "An example of loading the model, setting a loss criteria and defining an optimizer."
      ],
      "id": "alpha-custom"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "filled-davis"
      },
      "source": [
        "# Device configuration - defaults to CPU unless GPU is available on device\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "id": "filled-davis",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "optional-norfolk"
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "model = PolarNet().to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Stochastic gradient descent - TODO: alter as needed\n",
        "optimizer = optim.SGD(\n",
        "\tmodel.parameters(),\n",
        "\tlr=0.001,\n",
        "\tweight_decay=0.0005,\n",
        "\tmomentum=0.9,\n",
        ")"
      ],
      "id": "optional-norfolk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "differential-truth"
      },
      "source": [
        "Train the model, batch by batch, for as many iterations as required to converge. You can use the validation set to determine automatically when to stop training."
      ],
      "id": "differential-truth"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "greatest-motor"
      },
      "source": [
        ""
      ],
      "id": "greatest-motor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "therapeutic-pharmacology"
      },
      "source": [
        ""
      ],
      "id": "therapeutic-pharmacology",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pediatric-video"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Evaluate patch-based accuracy on the test set; then using the test patch coordinates, piece together the segmentation prediction on the original TIF images."
      ],
      "id": "pediatric-video"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "illegal-printer"
      },
      "source": [
        ""
      ],
      "id": "illegal-printer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "short-heritage"
      },
      "source": [
        ""
      ],
      "id": "short-heritage",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sharp-error"
      },
      "source": [
        ""
      ],
      "id": "sharp-error",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "accurate-ambassador"
      },
      "source": [
        ""
      ],
      "id": "accurate-ambassador",
      "execution_count": null,
      "outputs": []
    }
  ]
}